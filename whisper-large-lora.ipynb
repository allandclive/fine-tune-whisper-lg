{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# fine-tine whisper large with PEFT-LoRA + int8\n",
				"\n",
				"[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phineas-pta/fine-tune-whisper-vi/blob/main/whisper-large-lora.ipynb)\n",
				"\n",
				"on colab: mount gdrive using GUI before training\n",
				"\n",
				"on kaggle: select kaggle free P100 because complicated to run PEFT on multi-GPU\n",
				"\n",
				"https://huggingface.co/blog/fine-tune-whisper\n",
				"\n",
				"https://github.com/huggingface/peft/blob/main/examples/int8_training/peft_bnb_whisper_large_v2_training.ipynb"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from huggingface_hub import notebook_login\n",
				"notebook_login()\n",
				"# !huggingface-cli login --token=███"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# workaround for a bug in `datasets` package\n",
				"%pip install -q cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
				"%pip install -qU \"datasets[audio]\" accelerate transformers bitsandbytes peft\n",
				"# no compute metrics so no `jiwer`"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from dataclasses import dataclass\n",
				"import datasets as hugDS\n",
				"from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
				"from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"SAMPLING_RATE = 16_000\n",
				"def load_data(mode, **kwargs):\n",
				"\ttmp = hugDS.load_dataset(**kwargs, trust_remote_code=True, streaming=True).cast_column(\"audio\", hugDS.Audio(sampling_rate=SAMPLING_RATE))\n",
				"\tmatch mode:\n",
				"\t\tcase 0:\n",
				"\t\t\treturn tmp\n",
				"\t\tcase 1:\n",
				"\t\t\treturn tmp.select_columns([\"audio\", \"transcription\"])\n",
				"\t\tcase 2:\n",
				"\t\t\treturn tmp.select_columns([\"audio\", \"sentence\"]).rename_column(\"sentence\", \"transcription\")\n",
				"\t\tcase _:\n",
				"\t\t\traise ValueError(\"oh no!\")\n",
				"\n",
				"MY_DATA = hugDS.IterableDatasetDict()\n",
				"\n",
				"MY_DATA[\"train\"] = hugDS.concatenate_datasets([  # total: 1.4M samples\n",
				"\tload_data(path=\"doof-ferb/vlsp2020_vinai_100h\",                      split=\"train\", mode=0),  # 56.4k\n",
				"\tload_data(path=\"doof-ferb/fpt_fosd\",                                 split=\"train\", mode=0),  # 25.9k\n",
				"\tload_data(path=\"doof-ferb/infore1_25hours\",                          split=\"train\", mode=0),  # 14.9k\n",
				"\tload_data(path=\"doof-ferb/infore2_audiobooks\",                       split=\"train\", mode=0),  # 315k\n",
				"\tload_data(path=\"quocanh34/viet_vlsp\",                                split=\"train\", mode=0),  # 171k\n",
				"\tload_data(path=\"linhtran92/final_dataset_500hrs_wer0\",               split=\"train\", mode=1),  # 649k\n",
				"\tload_data(path=\"linhtran92/viet_youtube_asr_corpus_v2\",              split=\"train\", mode=1),  # 195k\n",
				"\tload_data(path=\"google/fleurs\",                        name=\"vi_vn\", split=\"train\", mode=1),  # 3k\n",
				"\tload_data(path=\"mozilla-foundation/common_voice_16_1\", name=\"vi\",    split=\"train\", mode=2),  # 2.3k\n",
				"\tload_data(path=\"vivos\",                                              split=\"train\", mode=2),  # 11.7k\n",
				"])\n",
				"\n",
				"MY_DATA[\"test\"] = hugDS.concatenate_datasets([  # total: 32.6k samples\n",
				"\tload_data(path=\"quocanh34/viet_vlsp\",                                split=\"validation\", mode=0),  # 7.5k\n",
				"\tload_data(path=\"linhtran92/viet_youtube_asr_corpus_v2\",              split=\"test\",       mode=1),  # 21.6k\n",
				"\tload_data(path=\"google/fleurs\",                        name=\"vi_vn\", split=\"validation\", mode=1),  # .3k\n",
				"\tload_data(path=\"google/fleurs\",                        name=\"vi_vn\", split=\"test\",       mode=1),  # .8k\n",
				"\tload_data(path=\"mozilla-foundation/common_voice_16_1\", name=\"vi\",    split=\"validation\", mode=2),  # .4k\n",
				"\tload_data(path=\"mozilla-foundation/common_voice_16_1\", name=\"vi\",    split=\"test\",       mode=2),  # 1.3k\n",
				"\tload_data(path=\"vivos\",                                              split=\"test\",       mode=2),  # .7k\n",
				"])\n",
				"\n",
				"# some samples will be filtered out later (unknown how many)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"modelID = \"openai/whisper-large-v3\"\n",
				"FEATURE_EXTRACTOR = WhisperFeatureExtractor.from_pretrained(modelID)\n",
				"TOKENIZER = WhisperTokenizer.from_pretrained(modelID, language=\"vi\", task=\"transcribe\")\n",
				"MODEL = WhisperForConditionalGeneration.from_pretrained(modelID, use_cache=False, forced_decoder_ids=None, suppress_tokens=[], load_in_8bit=True, device_map=\"auto\")\n",
				"\n",
				"DUMMY_TOKEN = -100"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"@dataclass\n",
				"class DataCollatorSpeechSeq2SeqWithPadding:\n",
				"\tdef __call__(self, features):\n",
				"\t\t# split inputs and labels since they have to be of different lengths and need different padding methods\n",
				"\t\tinput_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
				"\t\tlabel_features = [{\"input_ids\"     : feature[\"labels\"]        } for feature in features]  # get the tokenized label sequences\n",
				"\n",
				"\t\tbatch = FEATURE_EXTRACTOR.pad(input_features, return_tensors=\"pt\")  # treat the audio inputs by simply returning torch tensors\n",
				"\t\tlabels_batch =  TOKENIZER.pad(label_features, return_tensors=\"pt\")  # pad the labels to max length\n",
				"\t\tlabels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), DUMMY_TOKEN)  # replace padding with -100 to ignore loss correctly\n",
				"\n",
				"\t\tif (labels[:, 0] == TOKENIZER.bos_token_id).all().cpu().item():  # if bos token is appended in previous tokenization step,\n",
				"\t\t\tlabels = labels[:, 1:]  # cut bos token here as it’s append later anyways\n",
				"\n",
				"\t\tbatch[\"labels\"] = labels\n",
				"\t\treturn batch\n",
				"\n",
				"DATA_COLLATOR = DataCollatorSpeechSeq2SeqWithPadding()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def prepare_dataset(batch):\n",
				"\taudio = batch[\"audio\"]\n",
				"\tbatch[\"input_length\"] = len(audio)  # compute input length\n",
				"\tbatch[\"input_features\"] = FEATURE_EXTRACTOR(audio[\"array\"], sampling_rate=SAMPLING_RATE).input_features[0]  # compute log-Mel input features\n",
				"\tbatch[\"labels\"] = TOKENIZER(batch[\"transcription\"]).input_ids  # encode target text to label ids\n",
				"\tbatch[\"labels_length\"] = len(batch[\"labels\"])  # compute labels length\n",
				"\treturn batch\n",
				"\n",
				"def filter_inputs(input_length):\n",
				"\t\"\"\"Filter inputs with zero input length or longer than 30s\"\"\"\n",
				"\treturn 0 < input_length < 48e4  # 30s × 16kHz\n",
				"\n",
				"def filter_labels(labels_length):\n",
				"\t\"\"\"Filter label sequences longer than max length 448 tokens\"\"\"\n",
				"\treturn labels_length < 448  # MODEL.config.max_length\n",
				"\n",
				"MY_DATA = (MY_DATA\n",
				"\t# .shuffle(seed=42)  # useless coz streaming multiple datasets (cannot set buffer too high coz not enough RAM)\n",
				"\t.map(prepare_dataset)  # no `num_proc` coz streaming\n",
				"\t.filter(filter_inputs, input_columns= [\"input_length\"])  # no `remove_columns` coz streaming\n",
				"\t.filter(filter_labels, input_columns=[\"labels_length\"])  # no `remove_columns` coz streaming\n",
				")  # TODO: enable `batched=True` but don’t know how to write functions"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"MODEL_BIS = get_peft_model(\n",
				"\tprepare_model_for_int8_training(MODEL),\n",
				"\tLoraConfig(r=32, lora_alpha=64, target_modules=[\"fc1\", \"fc2\", \"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"], lora_dropout=.1, bias=\"none\")\n",
				")\n",
				"MODEL_BIS.print_trainable_parameters()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# mount gdrive using GUI before training\n",
				"%cd '/content/drive/My Drive/coder'\n",
				"# %cd /kaggle/working\n",
				"# !rm -rf ./my-whisper-large"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"SAVE_PATH = \"./my-whisper-large\"  # mount gdrive using GUI before training\n",
				"BATCH_SIZE = 16  # should be a multiple of 8\n",
				"# kaggle free P100 train faster than colab free T4\n",
				"# kaggle free T4×2: problem with peft + multi-gpu\n",
				"\n",
				"# colab free tier can only run for 8-12h max daily\n",
				"# kaggle free tier can only run for 30h max weekly but max 12h per session\n",
				"\n",
				"TRAINING_ARGS = Seq2SeqTrainingArguments(\n",
				"\toutput_dir=SAVE_PATH,\n",
				"\tper_device_train_batch_size=BATCH_SIZE,\n",
				"\tper_device_eval_batch_size=BATCH_SIZE,\n",
				"\tfp16=True,\n",
				"\t# bf16=True, tf32=True, torch_compile=True,  # GPU Ampere or later\n",
				"\treport_to=[\"tensorboard\"],\n",
				"\n",
				"\tmax_steps=500,  # no `num_train_epochs` coz streaming\n",
				"\twarmup_steps=50,\n",
				"\tlogging_steps=25,\n",
				"\tsave_steps=50,\n",
				"\teval_steps=50,\n",
				"\tevaluation_strategy=\"steps\",\n",
				"\tsave_total_limit=3,\n",
				"\n",
				"\tlearning_rate=1e-3,\n",
				"\twarmup_ratio=.05,  # keep between 5-15%\n",
				"\t# gradient_accumulation_steps=1,  # to increase if decrease batch size\n",
				"\tremove_unused_columns=False, label_names=[\"labels\"],  # required by PEFT\n",
				"\t# predict_with_generate=True,  # must disable coz PEFT\n",
				")\n",
				"\n",
				"TRAINER = Seq2SeqTrainer(\n",
				"\targs=TRAINING_ARGS,\n",
				"\tmodel=MODEL_BIS,\n",
				"\ttrain_dataset=MY_DATA[\"train\"],\n",
				"\teval_dataset=MY_DATA[\"test\"],\n",
				"\tdata_collator=DATA_COLLATOR,\n",
				"\t# compute_metrics=compute_metrics,  # must disable coz PEFT\n",
				"\ttokenizer=FEATURE_EXTRACTOR,  # not TOKENIZER\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"TRAINER.train()  # resume_from_checkpoint=True  # only if resume"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"TRAINER.save_model()\n",
				"!zip -FSr res.zip ./my-whisper-large"
			]
		}
	],
	"metadata": {
		"accelerator": "GPU",
		"colab": {
			"gpuType": "T4",
			"private_outputs": true,
			"provenance": []
		},
		"kaggle": {
			"accelerator": "nvidiaTeslaT4",
			"dataSources": [],
			"isGpuEnabled": true,
			"isInternetEnabled": true,
			"language": "python",
			"sourceType": "notebook"
		},
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 0
}
